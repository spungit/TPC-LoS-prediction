Model: LSTM
BaseLSTM(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (lstm_dropout): Dropout(p=0.2, inplace=False)
  (main_dropout): Dropout(p=0.45, inplace=False)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (empty_module): EmptyModule()
  (lstm): LSTM(176, 128, num_layers=2, dropout=0.2)
  (diagnosis_encoder): Linear(in_features=293, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_los): Linear(in_features=257, out_features=17, bias=True)
  (point_mort): Linear(in_features=257, out_features=17, bias=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
)
Number of training batches: 200.681640625
Epoch: 0 [    0/    0 samples] | train loss: 0.0000
Epoch: 0 [  512/  201 samples] | train loss: 0.8181
Epoch: 0 [ 1024/  401 samples] | train loss: 1.6196
Epoch: 0 [ 1536/  602 samples] | train loss: 2.4077
Epoch: 0 [ 2048/  803 samples] | train loss: 3.1949
Epoch: 0 [ 2560/ 1003 samples] | train loss: 3.9840
Epoch: 0 [ 3072/ 1204 samples] | train loss: 4.7507
Epoch: 0 [ 3584/ 1405 samples] | train loss: 5.5198
Epoch: 0 [ 4096/ 1605 samples] | train loss: 6.2769
Epoch: 0 [ 4608/ 1806 samples] | train loss: 7.0262
Epoch: 0 [ 5120/ 2007 samples] | train loss: 7.7647
Epoch: 0 [ 5632/ 2207 samples] | train loss: 8.5033
Epoch: 0 [ 6144/ 2408 samples] | train loss: 9.2307
Epoch: 0 [ 6656/ 2609 samples] | train loss: 9.9480
Epoch: 0 [ 7168/ 2810 samples] | train loss: 10.6616
Epoch: 0 [ 7680/ 3010 samples] | train loss: 11.3600
Epoch: 0 [ 8192/ 3211 samples] | train loss: 12.0596
Epoch: 0 [ 8704/ 3412 samples] | train loss: 12.7516
Epoch: 0 [ 9216/ 3612 samples] | train loss: 13.4404
Epoch: 0 [ 9728/ 3813 samples] | train loss: 14.1257
Epoch: 0 [10240/ 4014 samples] | train loss: 14.7988
Epoch: 0 [10752/ 4214 samples] | train loss: 15.4727
Epoch: 0 [11264/ 4415 samples] | train loss: 16.1148
Epoch: 0 [11776/ 4616 samples] | train loss: 16.7780
Epoch: 0 [12288/ 4816 samples] | train loss: 17.4350
Epoch: 0 [12800/ 5017 samples] | train loss: 18.0968
Epoch: 0 [13312/ 5218 samples] | train loss: 18.7373
Epoch: 0 [13824/ 5418 samples] | train loss: 19.3704
Epoch: 0 [14336/ 5619 samples] | train loss: 20.0082
Epoch: 0 [14848/ 5820 samples] | train loss: 20.6222
Epoch: 0 [15360/ 6020 samples] | train loss: 21.2440
Epoch: 0 [15872/ 6221 samples] | train loss: 21.8623
Epoch: 0 [16384/ 6422 samples] | train loss: 22.4679
Epoch: 0 [16896/ 6622 samples] | train loss: 23.0603
Epoch: 0 [17408/ 6823 samples] | train loss: 23.6349
Epoch: 0 [17920/ 7024 samples] | train loss: 24.2192
Epoch: 0 [18432/ 7225 samples] | train loss: 24.7830
Epoch: 0 [18944/ 7425 samples] | train loss: 25.3283
Epoch: 0 [19456/ 7626 samples] | train loss: 25.8858
Epoch: 0 [19968/ 7827 samples] | train loss: 26.4495
